name: Reproduce (matrix 256)
defaults:
  run:
    shell: bash

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - "src/**"
      - "scripts/**"
      - ".github/workflows/reproduce-matrix.yml"
      - ".github/workflows/reproduce.yml"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: matrix-reproduce-${{ github.ref }}
  cancel-in-progress: false

jobs:
  replicate:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        rate: [0.1, 0.3, 0.5, 0.7, 0.9]
        replica: [1,2,3,4,5]
    env:
      SOAK_SECONDS: "280"
      MISSING_LIVE_COUNT: "0"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Compute REPL_ID and RUN_DIR
        run: |
          RATE=${{ matrix.rate }}
          REPL=${{ matrix.replica }}
          echo "P_FAIL=${RATE}" >> $GITHUB_ENV
          echo "REPL_ID=${RATE}-${REPL}" >> $GITHUB_ENV
          echo "RUN_DIR=$GITHUB_WORKSPACE/runs/${RATE}/${REPL}" >> $GITHUB_ENV

      - name: Ensure run output dirs
        shell: bash
        run: |
          set -eu
          # required by later steps (site build and otel collector bind-mount)
          mkdir -p "${RUN_DIR}/collector"
          mkdir -p "${RUN_DIR}/site/data"

      - name: Install package (editable)
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e .
          sudo apt-get update && sudo apt-get install -y jq

      - name: Capture & Analyze
        shell: bash
        env:
          DEMO_DIR: ${{ github.workspace }}/vendor/opentelemetry-demo
          TARGET: ${{ matrix.target || 'otel-demo' }}
          SOAK_SECONDS: ${{ matrix.soak_seconds || env.SOAK_SECONDS }}
          SIZE_MB: ${{ matrix.traces_size_mb }}
        run: |
          set -euo pipefail
          mkdir -p "$RUN_DIR/collector"
          DEMO_DIR="${DEMO_DIR:-"$GITHUB_WORKSPACE/vendor/opentelemetry-demo"}"

          # Compose shim: prefer 'docker compose', fallback to classic 'docker-compose'
          if docker compose version >/dev/null 2>&1; then
            COMPOSE_CMD="docker compose"
          elif command -v docker-compose >/dev/null 2>&1; then
            COMPOSE_CMD="docker-compose"
          else
            echo "::error ::Neither 'docker compose' nor 'docker-compose' is available" >&2
            exit 1
          fi
          export COMPOSE_CMD
          echo "Using COMPOSE_CMD='${COMPOSE_CMD}'"

          # 1) Get the OpenTelemetry Demo at the fixed release v2.1.3 (deterministic)
          if [[ ! -d vendor/opentelemetry-demo ]]; then
            git clone https://github.com/open-telemetry/opentelemetry-demo vendor/opentelemetry-demo
          fi
          cd vendor/opentelemetry-demo
          git fetch --depth 1 origin refs/tags/2.1.3
          git checkout --detach FETCH_HEAD
          git submodule update --init --recursive
          cd "${GITHUB_WORKSPACE}"

          # 2) Enable a file exporter in the demo's "extras" config so traces are written to /data/otel-traces.json
          EXTRAS="${DEMO_DIR}/src/otel-collector/otelcol-config-extras.yml"
          
          # Drop only the exporters→service block to avoid conflicts, then append our config.
          awk 'BEGIN{p=1} /^exporters:/{p=0} /^service:/{print; p=1; next} p{print}' "$EXTRAS" > "$EXTRAS.tmp" && mv "$EXTRAS.tmp" "$EXTRAS"
          
          cat >> "$EXTRAS" <<'YAML'
          exporters:
            file/otlpjson:
              path: /data/otel-traces.json
              flush_interval: 2s
          
          service:
            pipelines:
              traces:
                exporters: [spanmetrics, otlp, file/otlpjson]
          YAML
        
          # 3) Compose override to mount a host directory for /data so the JSON lands in $RUN_DIR/collector
          # Resolve compose YAML path and then determine the load generator service key (loadgenerator vs load-generator)
          COMPOSE="${DEMO_DIR}/docker-compose.yml"
          # (awk reads the YAML file; do NOT pass the CLI here)
          LG=$(awk '/^[[:space:]]{2}(loadgenerator|load-generator):/{sub(":",""); gsub(" ",""); print $1; exit}' "$COMPOSE")
          LG=${LG:-loadgenerator}
          cat > "${DEMO_DIR}/docker-compose.override.yml" <<'YAML'
          services:
            otel-collector:
              volumes:
                - "${RUN_DIR}/collector:/data:rw"
            # Be resilient to upstream changes: ensure the load generator has an explicit image
            loadgenerator:
              image: ghcr.io/open-telemetry/demo-loadgenerator:latest
              command:
                - /bin/sh
                - -exc
                - |
                  p_fail="${P_FAIL}"
                  window=${LOCUST_WINDOW:-180}
                  failure_window=${FAILURE_WINDOW:-60}
                  case "$p_fail" in
                    0.1) chaos=("cartservice"); scale="0.9" ;;
                    0.3) chaos=("cartservice"); scale="0.7" ;;
                    0.5) chaos=("cartservice" "adservice"); scale="0.5" ;;
                    0.7) chaos=("cartservice" "adservice" "checkoutservice"); scale="0.3" ;;
                    0.9) chaos=("cartservice" "adservice" "checkoutservice" "frauddetectionservice"); scale="0.1" ;;
                    *) chaos=("cartservice"); scale="1.0" ;;
                  esac
                  echo "[loadgenerator] P_FAIL=$p_fail scale=$scale chaos=${chaos[*]} window=$window failure_window=$failure_window"
                  sleep 15
                  for svc in "${chaos[@]}"; do
                    docker compose -f /workspace/docker-compose.yml scale "$svc"="$scale" || true
                  done
                  locust --autostart --headless --users 50 --spawn-rate 5 --run-time "${window}s" --stop-timeout 90 --host http://frontend:8080 --csv /data/locust --csv-full-history
                  for svc in "${chaos[@]}"; do
                    docker compose -f /workspace/docker-compose.yml scale "$svc"=1 || true
                  done
              volumes:
                - "${RUN_DIR}:/data:rw"
            load-generator:
              image: ghcr.io/open-telemetry/demo-loadgenerator:latest
              command:
                - --autostart
                - --headless
                - --users
                - "50"
                - --spawn-rate
                - "5"
                - --run-time
                - "0"
                - --stop-timeout
                - "90"
                - --host
                - http://frontend:8080
                - --csv
                - /data/locust
                - --csv-full-history
              volumes:
                - "${RUN_DIR}:/data:rw"
          YAML

          echo "--- docker-compose.override.yml ---"
          nl -ba "${DEMO_DIR}/docker-compose.override.yml" | sed 's/^/  /'


        
          # 4) Bring the demo up, soak, then tear it down
          mkdir -p "${RUN_DIR}/collector"
          # ensure the collector (non-root in container) can create the file
          chmod 0777 "${RUN_DIR}"
          chmod 0777 "${RUN_DIR}/collector"
          # Create a dedicated dir for Locust CSVs and make it writable
          mkdir -p "${RUN_DIR}/locust"
          chmod 0777 "${RUN_DIR}/locust"

          COMPOSE="$DEMO_DIR/docker-compose.yml"
          COMPOSE_OVERRIDE="$DEMO_DIR/docker-compose.override.yml"
          COMPOSE_OPTS="-f $COMPOSE -f $COMPOSE_OVERRIDE"
          
          # Validate combined Compose config early to catch YAML/merge mistakes
          if ! $COMPOSE_CMD $COMPOSE_OPTS config >/dev/null; then
            echo "::error ::Compose config validation failed (base + override)." >&2
            echo "Base compose at: $COMPOSE"; echo "Override at: $COMPOSE_OVERRIDE";
            exit 1
          fi
          
          $COMPOSE_CMD $COMPOSE_OPTS up --force-recreate --remove-orphans --detach
          echo "Soaking for ${SOAK_SECONDS}s..."
          sleep "${SOAK_SECONDS}"
          
          test -s "${RUN_DIR}/collector/otel-traces.json" || {
            echo "ERROR: $(ls -lah ${RUN_DIR}/collector || true)"
            exit 1
          }

          # 4.1) Verify Locust is producing CSVs to the mounted host path BEFORE teardown
          echo "[locust] Waiting for Locust *_stats.csv to appear under ${RUN_DIR} (depth<=2)…"
          # Give it a grace period (≤ 60s) to flush files while still running
          for i in $(seq 1 60); do
            if find "${RUN_DIR}" -maxdepth 2 -type f -name '*_stats.csv' -print -quit 2>/dev/null | grep -q .; then
              echo "[locust] Found stats CSV(s):"
              find "${RUN_DIR}" -maxdepth 2 -type f -name '*_stats.csv' -print 2>/dev/null | sed 's#^#  #' || true
              break
            fi
            # Periodically show inside-container state to aid debugging
            if (( i % 5 == 0 )); then
              echo "[locust] Inspecting container '${LG}' /data contents…"
              $COMPOSE_CMD $COMPOSE_OPTS exec -T "${LG}" sh -lc 'echo "LOCUST_OPTS=${LOCUST_OPTS}"; ls -lah /data || true' || true
            fi
            sleep 1
          done
          # If still missing, fail early with context while containers are alive
          if ! find "${RUN_DIR}" -maxdepth 2 -type f -name '*_stats.csv' -print -quit 2>/dev/null | grep -q .; then
            echo "::error ::Locust did not produce *_stats.csv under ${RUN_DIR}. Live metrics are mandatory." >&2
            echo "[compose] ps:" && $COMPOSE_CMD $COMPOSE_OPTS ps || true
            echo "[compose] logs (${LG}):" && $COMPOSE_CMD $COMPOSE_OPTS logs "${LG}" || true
            exit 1
          fi

          $COMPOSE_CMD $COMPOSE_OPTS down -v || true

          # Discover Locust stats prefix (used later by live + entrypoint steps)
          STATS=$(find "${RUN_DIR}" -maxdepth 2 -type f -name '*_stats.csv' -print 2>/dev/null | sort -r | head -n1 || true)
          if [[ -z "${STATS}" || ! -s "${STATS}" ]]; then
            echo "::error ::No Locust *_stats.csv found under ${RUN_DIR} (depth<=2)." >&2
            exit 1
          fi
          LOCUST_PREFIX="${STATS%_stats.csv}"
          LOCUST_FAILURES="${STATS/_stats.csv/_failures.csv}"
          export LOCUST_PREFIX LOCUST_FAILURES STATS
          echo "LOCUST_PREFIX=${LOCUST_PREFIX}" >> "$GITHUB_ENV"
          echo "LOCUST_STATS=${STATS}" >> "$GITHUB_ENV"
          echo "LOCUST_FAILURES=${LOCUST_FAILURES}" >> "$GITHUB_ENV"
          echo "[locust] stats=${STATS}"
        
          # 5) Verify trace JSON and extract to Parquet
          if [[ ! -s "${RUN_DIR}/collector/otel-traces.json" ]]; then
          echo "ERROR: ${RUN_DIR}/collector/otel-traces.json is missing or empty."
          ls -l "${RUN_DIR}/collector" || true
          exit 1
          fi
          edgetyper extract --input "${RUN_DIR}/collector/otel-traces.json" --out "${RUN_DIR}/spans.parquet"
          echo "[capture] spans.parquet: $(ls -lh "${RUN_DIR}/spans.parquet" | awk '{print $5}')"
        
          # 6) Continue exactly as before (graph → features → predictions → eval → plan → observe)
          edgetyper graph \
          --spans "${RUN_DIR}/spans.parquet" \
          --out-events "${RUN_DIR}/events.parquet" \
          --out-edges "${RUN_DIR}/edges.parquet" \
          --with-broker-edges
          
          edgetyper featurize \
          --events "${RUN_DIR}/events.parquet" \
          --edges  "${RUN_DIR}/edges.parquet" \
          --out    "${RUN_DIR}/features.parquet"
          
          edgetyper baseline --features "${RUN_DIR}/features.parquet" --mode semconv --out "${RUN_DIR}/pred_semconv.csv"
          edgetyper baseline --features "${RUN_DIR}/features.parquet" --mode timing  --out "${RUN_DIR}/pred_timing.csv"
          edgetyper label    --features "${RUN_DIR}/features.parquet"              --out "${RUN_DIR}/pred_ours.csv"
          # Export graph.json (service graph + features + predictions)
          python scripts/export_graph_json.py \
            --edges "${RUN_DIR}/edges.parquet" \
            --features "${RUN_DIR}/features.parquet" \
            --pred "${RUN_DIR}/pred_ours.csv" \
            --out "${RUN_DIR}/graph.json"

          # Fixed entrypoints + live-target mapping for this experiment
          cp config/entrypoints.txt "${RUN_DIR}/entrypoints.txt"
          cp config/live_targets.yaml "${RUN_DIR}/live_targets.yaml"
          echo "[entrypoints.txt]" && cat "${RUN_DIR}/entrypoints.txt" || true

          # Audit: compare entrypoints vs edges.parquet services (first 5 present/missing)
          python - <<'PY'
          import pandas as pd, os
          run = os.environ['RUN_DIR']
          eps = [ln.strip() for ln in open(f"{run}/entrypoints.txt", 'r', encoding='utf-8').read().splitlines() if ln.strip()]
          try:
              edges = pd.read_parquet(f"{run}/edges.parquet")
              sv = pd.unique(pd.concat([edges['src_service'].astype(str), edges['dst_service'].astype(str)])).tolist()
          except Exception:
              sv = []
          present = [e for e in eps if e in sv][:5]
          missing = [e for e in eps if e not in sv][:5]
          print(f"[audit] entrypoints passed to resilience: entrypoints.txt")
          print(f"[audit] edges services (first 10): {sv[:10]}")
          print(f"[audit] entrypoints present in graph (first 5): {present}")
          print(f"[audit] entrypoints NOT found in graph (first 5): {missing}")
          PY
          
          # Robustness masks
          edgetyper featurize --events "${RUN_DIR}/events.parquet" --edges "${RUN_DIR}/edges.parquet" --out "${RUN_DIR}/features_semconv_drop.parquet" --mask-semconv
          edgetyper label    --features "${RUN_DIR}/features_semconv_drop.parquet" --out "${RUN_DIR}/pred_ours_semconv_drop.csv" --uncertain-threshold 0.55
          
          edgetyper featurize --events "${RUN_DIR}/events.parquet" --edges "${RUN_DIR}/edges.parquet" --out "${RUN_DIR}/features_timing_drop.parquet" --mask-timing
          edgetyper label    --features "${RUN_DIR}/features_timing_drop.parquet" --out "${RUN_DIR}/pred_ours_timing_drop.csv" --uncertain-threshold 0.55
          
          # Physical GT path in this repo
          GT="src/edgetyper/ground_truth.yaml"
          
          edgetyper eval --name "Baseline — SemConv (physical)"   --pred "${RUN_DIR}/pred_semconv.csv"             --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_semconv_physical.json"
          edgetyper eval --name "Baseline — Timing (physical)"    --pred "${RUN_DIR}/pred_timing.csv"              --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_timing_physical.json"
          edgetyper eval --name "EdgeTyper (ours) — physical"     --pred "${RUN_DIR}/pred_ours.csv"                --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_ours_physical.json"
          edgetyper eval --name "EdgeTyper (ours) — SemConv dropped" --pred "${RUN_DIR}/pred_ours_semconv_drop.csv" --features "${RUN_DIR}/features_semconv_drop.parquet" --gt "$GT" --out "${RUN_DIR}/metrics_ours_semconv_drop.json"
          edgetyper eval --name "EdgeTyper (ours) — Timing dropped"  --pred "${RUN_DIR}/pred_ours_timing_drop.csv"  --features "${RUN_DIR}/features_timing_drop.parquet"  --gt "$GT" --out "${RUN_DIR}/metrics_ours_timing_drop.json"
          
          # Plans: ours vs all-blocking synthetic
          edgetyper plan --edges "${RUN_DIR}/edges.parquet" --pred "${RUN_DIR}/pred_ours.csv" --out "${RUN_DIR}/plan_physical.csv" --weight events --alpha-ack 1.0
          python - "${RUN_DIR}/edges.parquet" "${RUN_DIR}/pred_all_blocking.csv" <<'PY'
          import pandas as pd, sys
          edges = pd.read_parquet(sys.argv[1])[["src_service","dst_service"]].drop_duplicates()
          edges["pred_label"]="sync"
          edges.to_csv(sys.argv[2], index=False)
          PY
          edgetyper plan --edges "${RUN_DIR}/edges.parquet" --pred "${RUN_DIR}/pred_all_blocking.csv" --out "${RUN_DIR}/plan_all_blocking.csv" --weight events --alpha-ack 1.0
          
          # Observe: build equal windows (baseline and two fault windows)
          python - <<'PY'
          import json, pandas as pd, os
          from pathlib import Path
          run = Path(os.environ["RUN_DIR"])
          sp  = pd.read_parquet(run/"spans.parquet")
          ts  = [c for c in ["start_time_unix_nano","start_unix_nano","start_ns","time_unix_nano"] if c in sp.columns][0]
          tmin, tmax = int(sp[ts].min()), int(sp[ts].max())
          SOAK = int(os.environ.get("SOAK_SECONDS", "180"))
          W = max(1, SOAK // 3) * 1_000_000_000  # three equal windows within soak
          windows = [( "baseline", None, 3), ("fault:kafka","kafka",2), ("fault:consumer","fraud-detection",1)]
          segments=[]
          for name,target,slot in windows:
              start = max(tmin, tmax - slot*W)
              end   = max(start+1, min(tmax, start+W))
              seg = {"name": name, "start_ns": start, "end_ns": end}
              if target: seg["target_service"] = target
              segments.append(seg)
          (run/"segments.json").write_text(json.dumps({"segments":segments}, indent=2))
          PY
          edgetyper observe --spans "${RUN_DIR}/spans.parquet" --segments "${RUN_DIR}/segments.json" --out "${RUN_DIR}/observations.json"
      
          # Provenance (report size from JSON or fallback to Parquet)
          SIZE_MB=$(python - <<'PY'
          import os, json
          run=os.environ["RUN_DIR"]
          tj=os.path.join(run,"collector","otel-traces.json")
          pq=os.path.join(run,"spans.parquet")
          sz=os.path.getsize(tj) if os.path.exists(tj) else (os.path.getsize(pq) if os.path.exists(pq) else 0)
          print(round(sz/1024/1024,2))
          PY
          )
          
          jq -n \
            --arg  target "$TARGET" \
            --argjson soak "$SOAK_SECONDS" \
            --argjson size "$SIZE_MB" \
            '{target: $target, soak_seconds: $soak, traces_size_mb: $size}' \
            > "${RUN_DIR}/provenance.json"

      - name: Monte‑Carlo availability (typed)
        run: |
          edgetyper resilience \
            --edges "${RUN_DIR}/edges.parquet" \
            --pred  "${RUN_DIR}/pred_ours.csv" \
            --entrypoints "${RUN_DIR}/entrypoints.txt" \
            --p-fail 0.1 --p-fail 0.3 --p-fail 0.5 --p-fail 0.7 --p-fail 0.9 \
            --samples 900000 \
            --out "${RUN_DIR}/availability_typed.csv"
          echo "[typed] lines: $(wc -l < "${RUN_DIR}/availability_typed.csv" 2>/dev/null || echo 0)"
          echo "[typed] head:" && head -n 10 "${RUN_DIR}/availability_typed.csv" || true

      - name: Monte‑Carlo availability (all‑blocking)
        run: |
          edgetyper resilience \
            --edges "${RUN_DIR}/edges.parquet" \
            --pred  "${RUN_DIR}/pred_ours.csv" \
            --assume-all-blocking \
            --entrypoints "${RUN_DIR}/entrypoints.txt" \
            --p-fail 0.1 --p-fail 0.3 --p-fail 0.5 --p-fail 0.7 --p-fail 0.9 \
            --samples 900000 \
            --out "${RUN_DIR}/availability_block.csv"
          echo "[all-block] lines: $(wc -l < "${RUN_DIR}/availability_block.csv" 2>/dev/null || echo 0)"
          echo "[all-block] head:" && head -n 10 "${RUN_DIR}/availability_block.csv" || true

      - name: Extended verification (labels & availability sanity)
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          import pandas as pd

          run = Path(os.environ["RUN_DIR"])
          eps_txt = run / "entrypoints.txt"
          eps = {ln.strip() for ln in eps_txt.read_text(encoding="utf-8").splitlines() if ln.strip()}
          if not eps:
              raise SystemExit("entrypoints.txt is empty after generation.")
          print(f"[verify] entrypoints discovered: {sorted(eps)}")

          preds = pd.read_csv(run / "pred_ours.csv")
          counts = preds["pred_label"].value_counts()
          async_count = int(counts.get("async", 0))
          sync_count = int(counts.get("sync", 0))
          uncertain_count = int(counts.get("uncertain", 0))
          print(f"[verify] label counts → async={async_count} sync={sync_count} uncertain={uncertain_count}")
          if async_count == 0:
              raise SystemExit("No async edges predicted — typed graph collapses to all-blocking.")

          typed = pd.read_csv(run / "availability_typed.csv")
          block = pd.read_csv(run / "availability_block.csv")
          merged = typed.merge(block, on=["entrypoint", "p_fail"], suffixes=("_typed", "_block"))
          if merged.empty:
              raise SystemExit("Availability CSVs merged to zero rows; check entrypoints/p_fail grid.")
          merged["diff"] = (merged["R_model_typed"] - merged["R_model_block"]).abs()
          max_diff = float(merged["diff"].max())
          print(f"[verify] max |typed - all-block| = {max_diff:.6f}")
          if max_diff < 1e-6:
              print("[verify] WARNING: typed availability matches all-block nearly exactly (max diff < 1e-6).")
          PY

      - name: Build live availability from Locust CSV (mandatory)
        run: |
          set -euo pipefail
          STATS="${LOCUST_STATS}"
          if [[ -z "${STATS}" || ! -s "${STATS}" ]]; then
            echo "::error ::[live] LOCUST_STATS is missing or empty. Ensure discovery step succeeded." >&2
            exit 1
          fi
          FAILS="${LOCUST_FAILURES}"
          echo "[live] Using stats=${STATS} failures=${FAILS}"
          python scripts/build_live_availability.py \
            --stats       "${STATS}" \
            --failures    "${FAILS}" \
            --entrypoints "${RUN_DIR}/entrypoints.txt" \
            --targets     "${RUN_DIR}/live_targets.yaml" \
            --replica     "replicate-${REPL_ID}" \
            --p-grid      "${P_FAIL}" \
            --append \
            --no-strict \
            --out         "${RUN_DIR}/live_availability.csv"


      - name: Validate live vs model grids
        run: |
          python - <<'PY'
          import os
          from pathlib import Path
          import pandas as pd

          run = Path(os.environ["RUN_DIR"])
          env_path = Path(os.environ["GITHUB_ENV"])
          typed = pd.read_csv(run / "availability_typed.csv")
          live = pd.read_csv(run / "live_availability.csv")

          if typed.empty:
              raise SystemExit("availability_typed.csv is empty; model did not produce any rows.")
          if live.empty:
              raise SystemExit("live_availability.csv is empty; no Locust measurements recorded.")

          def _pairs(df):
              return {
                  (str(r["entrypoint"]), float(r["p_fail"]))
                  for _, r in df.iterrows()
              }

          typed_pairs = _pairs(typed)
          live_pairs = _pairs(live)
          missing = sorted(typed_pairs - live_pairs)
          miss_path = run / "missing_live_entrypoints.txt"
          if missing:
              examples = ", ".join(f"{e}:{p}" for e, p in missing[:5])
              miss_path.write_text("\n".join(f"{e},{p}" for e, p in missing), encoding="utf-8")
              print(
                  f"::warning ::Live availability missing {len(missing)} entrypoint×p_fail rows; first few: {examples}. "
                  f"Full list written to {miss_path.name}."
              )
          else:
              if miss_path.exists():
                  miss_path.unlink()
              print(f"[validate] Live grid matches model ({len(live_pairs)} rows).")

          with env_path.open("a", encoding="utf-8") as fh:
              fh.write(f"MISSING_LIVE_COUNT={len(missing)}\n")
          PY

          # Ensure the file exists and contains data (header + ≥1 row)
          if [[ ! -s "${RUN_DIR}/live_availability.csv" ]]; then
            echo "::error ::[live] live_availability.csv was not produced or is empty." >&2
            exit 1
          fi
          LINES=$(wc -l < "${RUN_DIR}/live_availability.csv" || echo 0)
          if [[ "${LINES}" -le 1 ]]; then
            echo "::error ::[live] live_availability.csv has no data rows (lines=${LINES})." >&2
            head -n 10 "${RUN_DIR}/live_availability.csv" || true
            exit 1
          fi
          echo "[live] Sample output:" && head -n 10 "${RUN_DIR}/live_availability.csv"
          LIVE_ARGS=(--live "${RUN_DIR}/live_availability.csv")

          # Always produce availability.json (typed/all-block; include live if present)
          python scripts/export_availability_json.py \
            --typed    "${RUN_DIR}/availability_typed.csv" \
            --blocking "${RUN_DIR}/availability_block.csv" \
            "${LIVE_ARGS[@]}" \
            --out     "${RUN_DIR}/availability.json"

      - name: Snapshot diagnostics
        run: |
          python - <<'PY'
          import json, os, pandas as pd
          from pathlib import Path

          run = Path(os.environ["RUN_DIR"])
          diag = {
              "entrypoints": (run / "entrypoints.txt").read_text(encoding="utf-8").split(),
              "availability": {},
          }
          for label, fname in (
              ("typed", "availability_typed.csv"),
              ("all_block", "availability_block.csv"),
              ("live", "live_availability.csv"),
          ):
              path = run / fname
              if path.exists() and path.stat().st_size:
                  try:
                      df = pd.read_csv(path)
                      diag["availability"][label] = {
                          "path": str(path),
                          "rows": int(len(df)),
                          "entrypoints": sorted(set(df["entrypoint"].dropna().astype(str))) if "entrypoint" in df.columns else [],
                      }
                  except Exception as exc:
                      diag["availability"][label] = {"path": str(path), "error": str(exc)}
              else:
                  diag["availability"][label] = {"path": str(path), "missing": True}
          stats = run / "locust_stats.csv"
          if stats.exists():
              diag["locust_stats"] = {"path": str(stats), "lines": sum(1 for _ in stats.open())}
          (run / "diagnostics.json").write_text(json.dumps(diag, indent=2), encoding="utf-8")
          print(json.dumps(diag, indent=2))
          PY

      - name: Upload replica artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: replicate-${{ env.REPL_ID }}
          if-no-files-found: warn
          path: |
            ${{ env.RUN_DIR }}/metrics_*.json
            ${{ env.RUN_DIR }}/plan_physical.csv
            ${{ env.RUN_DIR }}/plan_all_blocking.csv
            ${{ env.RUN_DIR }}/availability_typed.csv
            ${{ env.RUN_DIR }}/availability_block.csv
            ${{ env.RUN_DIR }}/availability.json
            ${{ env.RUN_DIR }}/graph.json
            ${{ env.RUN_DIR }}/live_availability.csv
            ${{ env.RUN_DIR }}/observations.json
            ${{ env.RUN_DIR }}/provenance.json
            ${{ env.RUN_DIR }}/live_availability.csv
            ${{ env.RUN_DIR }}/locust*_stats*.csv
            ${{ env.RUN_DIR }}/locust*_failures*.csv
            ${{ env.RUN_DIR }}/edges.parquet
            ${{ env.RUN_DIR }}/pred_ours.csv
            ${{ env.RUN_DIR }}/entrypoints.txt
            ${{ env.RUN_DIR }}/live_targets.yaml
            ${{ env.RUN_DIR }}/missing_live_entrypoints.txt

      - name: Build report site
        run: python -m edgetyper.cli report --metrics "${{ env.RUN_DIR }}" --out site

  aggregate:
    runs-on: ubuntu-latest
    needs: [replicate]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install aggregator dependencies
        run: |
          python -m pip install --upgrade pip
          # EITHER pin to your requirements file (recommended)
          if [ -f scripts/requirements-aggregate.txt ]; then
            pip install -r scripts/requirements-aggregate.txt
          elif [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install "pandas>=2.2,<2.4" "scipy>=1.11,<1.16" "jinja2>=3.1"
          fi    

      - name: Download all replica artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: replicate-*
          path: replicas

      - name: Sanity-check downloaded artifacts
        run: |
          echo "=== tree replicas (depth<=2) ==="
          find replicas -maxdepth 2 -type f -print | sed 's#^#  #' 
          echo "=== metrics present? ==="
          find replicas -type f \( -name 'observations.json' -o -name 'metrics_*json' \) -print | sed 's#^#  #'
          echo "=== availability files discovered recursively ==="
          find replicas -type f -name 'availability_*' -print | sed 's#^#  #'

      - name: Validate live grid completeness (aggregate)
        run: |
          python - <<'PY'
          from pathlib import Path
          import pandas as pd, sys

          expected_p = [0.1, 0.3, 0.5, 0.7, 0.9]
          entrypoints = [ln.strip() for ln in Path("config/entrypoints.txt").read_text().splitlines() if ln.strip()]
          expected = {(ep, float(p)) for ep in entrypoints for p in expected_p}

          seen = set()
          for csv_path in Path("replicas").glob("replicate-*/live_availability.csv"):
              df = pd.read_csv(csv_path)
              if df.empty:
                  continue
              for entry in df.itertuples(index=False):
                  try:
                      ep = str(entry.entrypoint).strip()
                      p = float(entry.p_fail)
                  except Exception:
                      continue
                  seen.add((ep, p))

          missing = sorted(expected - seen)
          if missing:
              msg = ", ".join(f"{ep}:{p}" for ep, p in missing)
              raise SystemExit(f"Live availability missing {len(missing)} entrypoint×p_fail rows overall; missing: {msg}")
          PY

      - name: Aggregate replicas → site/
        run: |
          AVAIL_ONLY=1 python scripts/aggregate_replicas.py --replicas-dir replicas --outdir site

      - name: Preview aggregate outputs
        run: |
          set -euo pipefail
          echo "[aggregate] Listing site/:"
          ls -lah site || true
          echo "[aggregate] Listing site/data:"
          ls -lah site/data || true
          echo "[aggregate] Preview CSVs:"
          for f in site/data/*.csv; do
            echo "-- $f"
            head -n 10 "$f" || true
          done

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: [aggregate]
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
