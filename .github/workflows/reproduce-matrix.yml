name: Reproduce (matrix 256)
defaults:
  run:
    shell: bash

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - "src/**"
      - "scripts/**"
      - ".github/workflows/reproduce-matrix.yml"
      - ".github/workflows/reproduce.yml"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: matrix-reproduce-${{ github.ref }}
  cancel-in-progress: false

jobs:
  replicate:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        i: [1,2,3,4,5] #,6,7,8,9,10,11,12,13,14,15,16]
        j: [1,2,3,4,5] #,6,7,8,9,10,11,12,13,14,15,16]
    env:
      SOAK_SECONDS: "180"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Compute REPL_ID and RUN_DIR
        run: |
          I=${{ matrix.i }}
          J=${{ matrix.j }}
          REPL_ID=$(( ( (I-1)*16 ) + J ))
          echo "REPL_ID=$REPL_ID" >> $GITHUB_ENV
          echo "RUN_DIR=$GITHUB_WORKSPACE/runs/${REPL_ID}" >> $GITHUB_ENV

      - name: Ensure run output dirs
        shell: bash
        run: |
          set -eu
          # required by later steps (site build and otel collector bind-mount)
          mkdir -p "${RUN_DIR}/collector"
          mkdir -p "${RUN_DIR}/site/data"

      - name: Install package (editable)
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e .
          sudo apt-get update && sudo apt-get install -y jq

      - name: Capture & Analyze
        shell: bash
        env:
          DEMO_DIR: ${{ github.workspace }}/vendor/opentelemetry-demo
          TARGET: ${{ matrix.target || 'otel-demo' }}
          SOAK_SECONDS: ${{ matrix.soak_seconds || env.SOAK_SECONDS }}
          SIZE_MB: ${{ matrix.traces_size_mb }}
        run: |
          set -euo pipefail
          mkdir -p "$RUN_DIR/collector"
          DEMO_DIR="${DEMO_DIR:-"$GITHUB_WORKSPACE/vendor/opentelemetry-demo"}"

          # 1) Get the OpenTelemetry Demo at a shallow depth (fast, deterministic)
          if [[ ! -d vendor/opentelemetry-demo ]]; then
            git clone --depth=1 https://github.com/open-telemetry/opentelemetry-demo vendor/opentelemetry-demo
          fi

          # 2) Enable a file exporter in the demo's "extras" config so traces are written to /data/otel-traces.json
          EXTRAS="${DEMO_DIR}/src/otel-collector/otelcol-config-extras.yml"
          
          # Drop only the exporters→service block to avoid conflicts, then append our config.
          awk 'BEGIN{p=1} /^exporters:/{p=0} /^service:/{print; p=1; next} p{print}' "$EXTRAS" > "$EXTRAS.tmp" && mv "$EXTRAS.tmp" "$EXTRAS"
          
          cat >> "$EXTRAS" <<'YAML'
          exporters:
            file/otlpjson:
              path: /data/otel-traces.json
              flush_interval: 2s
          
          service:
            pipelines:
              traces:
                exporters: [spanmetrics, otlp, file/otlpjson]
          YAML
        
          # 3) Compose override to mount a host directory for /data so the JSON lands in $RUN_DIR/collector
          cat > "${DEMO_DIR}/docker-compose.override.yml" <<'YAML'
          services:
            otel-collector:
              volumes:
                - "${RUN_DIR}/collector:/data:rw"
          YAML

        
          # 4) Bring the demo up, soak, then tear it down
          mkdir -p "${RUN_DIR}/collector"
          # ensure the collector (non-root in container) can create the file
          chmod 0777 "${RUN_DIR}/collector"

          COMPOSE="$DEMO_DIR/docker-compose.yml"
          COMPOSE_OVERRIDE="$DEMO_DIR/docker-compose.override.yml"
          COMPOSE_OPTS="-f $COMPOSE -f $COMPOSE_OVERRIDE"
          
          docker compose $COMPOSE_OPTS up --force-recreate --remove-orphans --detach
          echo "Soaking for ${SOAK_SECONDS}s..."
          sleep "${SOAK_SECONDS}"
          
          test -s "${RUN_DIR}/collector/otel-traces.json" || {
            echo "ERROR: $(ls -lah ${RUN_DIR}/collector || true)"
            exit 1
          }

          docker compose $COMPOSE_OPTS down -v || true
        
          # 5) Verify trace JSON and extract to Parquet
          if [[ ! -s "${RUN_DIR}/collector/otel-traces.json" ]]; then
          echo "ERROR: ${RUN_DIR}/collector/otel-traces.json is missing or empty."
          ls -l "${RUN_DIR}/collector" || true
          exit 1
          fi
          edgetyper extract --input "${RUN_DIR}/collector/otel-traces.json" --out "${RUN_DIR}/spans.parquet"
          echo "[capture] spans.parquet: $(ls -lh "${RUN_DIR}/spans.parquet" | awk '{print $5}')"
        
          # 6) Continue exactly as before (graph → features → predictions → eval → plan → observe)
          edgetyper graph \
          --spans "${RUN_DIR}/spans.parquet" \
          --out-events "${RUN_DIR}/events.parquet" \
          --out-edges "${RUN_DIR}/edges.parquet" \
          --with-broker-edges
          
          edgetyper featurize \
          --events "${RUN_DIR}/events.parquet" \
          --edges  "${RUN_DIR}/edges.parquet" \
          --out    "${RUN_DIR}/features.parquet"
          
          edgetyper baseline --features "${RUN_DIR}/features.parquet" --mode semconv --out "${RUN_DIR}/pred_semconv.csv"
          edgetyper baseline --features "${RUN_DIR}/features.parquet" --mode timing  --out "${RUN_DIR}/pred_timing.csv"
          edgetyper label    --features "${RUN_DIR}/features.parquet"              --out "${RUN_DIR}/pred_ours.csv"
          
          # Robustness masks
          edgetyper featurize --events "${RUN_DIR}/events.parquet" --edges "${RUN_DIR}/edges.parquet" --out "${RUN_DIR}/features_semconv_drop.parquet" --mask-semconv
          edgetyper label    --features "${RUN_DIR}/features_semconv_drop.parquet" --out "${RUN_DIR}/pred_ours_semconv_drop.csv" --uncertain-threshold 0.55
          
          edgetyper featurize --events "${RUN_DIR}/events.parquet" --edges "${RUN_DIR}/edges.parquet" --out "${RUN_DIR}/features_timing_drop.parquet" --mask-timing
          edgetyper label    --features "${RUN_DIR}/features_timing_drop.parquet" --out "${RUN_DIR}/pred_ours_timing_drop.csv" --uncertain-threshold 0.55
          
          # Physical GT path in this repo
          GT="src/edgetyper/ground_truth.yaml"
          
          edgetyper eval --name "Baseline — SemConv (physical)"   --pred "${RUN_DIR}/pred_semconv.csv"             --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_semconv_physical.json"
          edgetyper eval --name "Baseline — Timing (physical)"    --pred "${RUN_DIR}/pred_timing.csv"              --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_timing_physical.json"
          edgetyper eval --name "EdgeTyper (ours) — physical"     --pred "${RUN_DIR}/pred_ours.csv"                --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_ours_physical.json"
          edgetyper eval --name "EdgeTyper (ours) — SemConv dropped" --pred "${RUN_DIR}/pred_ours_semconv_drop.csv" --features "${RUN_DIR}/features_semconv_drop.parquet" --gt "$GT" --out "${RUN_DIR}/metrics_ours_semconv_drop.json"
          edgetyper eval --name "EdgeTyper (ours) — Timing dropped"  --pred "${RUN_DIR}/pred_ours_timing_drop.csv"  --features "${RUN_DIR}/features_timing_drop.parquet"  --gt "$GT" --out "${RUN_DIR}/metrics_ours_timing_drop.json"
          
          # Plans: ours vs all-blocking synthetic
          edgetyper plan --edges "${RUN_DIR}/edges.parquet" --pred "${RUN_DIR}/pred_ours.csv" --out "${RUN_DIR}/plan_physical.csv" --weight events --alpha-ack 1.0
          python - "${RUN_DIR}/edges.parquet" "${RUN_DIR}/pred_all_blocking.csv" <<'PY'
          import pandas as pd, sys
          edges = pd.read_parquet(sys.argv[1])[["src_service","dst_service"]].drop_duplicates()
          edges["pred_label"]="sync"
          edges.to_csv(sys.argv[2], index=False)
          PY
          edgetyper plan --edges "${RUN_DIR}/edges.parquet" --pred "${RUN_DIR}/pred_all_blocking.csv" --out "${RUN_DIR}/plan_all_blocking.csv" --weight events --alpha-ack 1.0
          
          # Observe: build equal windows (baseline and two fault windows)
          python - <<'PY'
          import json, pandas as pd, os
          from pathlib import Path
          run = Path(os.environ["RUN_DIR"])
          sp  = pd.read_parquet(run/"spans.parquet")
          ts  = [c for c in ["start_time_unix_nano","start_unix_nano","start_ns","time_unix_nano"] if c in sp.columns][0]
          tmin, tmax = int(sp[ts].min()), int(sp[ts].max())
          SOAK = int(os.environ.get("SOAK_SECONDS", "180"))
          W = max(1, SOAK // 3) * 1_000_000_000  # three equal windows within soak
          windows = [( "baseline", None, 3), ("fault:kafka","kafka",2), ("fault:consumer","fraud-detection",1)]
          segments=[]
          for name,target,slot in windows:
              start = max(tmin, tmax - slot*W)
              end   = max(start+1, min(tmax, start+W))
              seg = {"name": name, "start_ns": start, "end_ns": end}
              if target: seg["target_service"] = target
              segments.append(seg)
          (run/"segments.json").write_text(json.dumps({"segments":segments}, indent=2))
          PY
          edgetyper observe --spans "${RUN_DIR}/spans.parquet" --segments "${RUN_DIR}/segments.json" --out "${RUN_DIR}/observations.json"
      
          # Provenance (report size from JSON or fallback to Parquet)
          SIZE_MB=$(python - <<'PY'
          import os, json
          run=os.environ["RUN_DIR"]
          tj=os.path.join(run,"collector","otel-traces.json")
          pq=os.path.join(run,"spans.parquet")
          sz=os.path.getsize(tj) if os.path.exists(tj) else (os.path.getsize(pq) if os.path.exists(pq) else 0)
          print(round(sz/1024/1024,2))
          PY
          )
          
          jq -n \
            --arg  target "$TARGET" \
            --argjson soak "$SOAK_SECONDS" \
            --argjson size "$SIZE_MB" \
            '{target: $target, soak_seconds: $soak, traces_size_mb: $size}' \
            > "${RUN_DIR}/provenance.json"

      - name: Monte‑Carlo availability (typed)
        run: |
          edgetyper resilience \
            --edges "${RUN_DIR}/edges.parquet" \
            --pred  "${RUN_DIR}/pred_ours.csv" \
            --p-fail 0.1 --p-fail 0.3 --p-fail 0.5 --p-fail 0.7 --p-fail 0.9 \
            --samples 900000 \
            --out "${RUN_DIR}/availability_typed.csv"

      - name: Monte‑Carlo availability (all‑blocking)
        run: |
          edgetyper resilience \
            --edges "${RUN_DIR}/edges.parquet" \
            --pred  "${RUN_DIR}/pred_ours.csv" \
            --assume-all-blocking \
            --p-fail 0.1 --p-fail 0.3 --p-fail 0.5 --p-fail 0.7 --p-fail 0.9 \
            --samples 900000 \
            --out "${RUN_DIR}/availability_block.csv"

      - name: Upload replica artifact
        uses: actions/upload-artifact@v4
        with:
          name: replicate-${{ env.REPL_ID }}
          path: |
            ${{ env.RUN_DIR }}/metrics_*.json
            ${{ env.RUN_DIR }}/plan_physical.csv
            ${{ env.RUN_DIR }}/plan_all_blocking.csv
            ${{ env.RUN_DIR }}/observations.json
            ${{ env.RUN_DIR }}/provenance.json

      - name: Build report site
        run: python -m edgetyper.cli report --metrics "${{ env.RUN_DIR }}" --out site

  aggregate:
    runs-on: ubuntu-latest
    needs: [replicate]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install aggregator dependencies
        run: |
          python -m pip install --upgrade pip
          # EITHER pin to your requirements file (recommended)
          if [ -f scripts/requirements-aggregate.txt ]; then
            pip install -r scripts/requirements-aggregate.txt
          elif [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            pip install "pandas>=2.2,<2.4" "scipy>=1.11,<1.16" "jinja2>=3.1"
          fi    

      - name: Download all replica artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: replicate-*
          path: replicas

      - name: Sanity-check downloaded artifacts
        run: |
          echo "=== tree replicas ==="
          find replicas -maxdepth 2 -type f -print | sed 's#^#  #'
          echo "=== metrics present? ==="
          find replicas -type f \( -name 'observations.json' -o -name 'metrics_*json' \) -print | sed 's#^#  #'

      - name: Aggregate replicas → site/
        run: |
          python scripts/aggregate_replicas.py --replicas-dir replicas --outdir site

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: [aggregate]
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
