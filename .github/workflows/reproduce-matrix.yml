name: Reproduce (matrix 256)

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - "src/**"
      - "scripts/**"
      - ".github/workflows/reproduce-matrix.yml"
      - ".github/workflows/reproduce.yml"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: matrix-reproduce-${{ github.ref }}
  cancel-in-progress: false

jobs:
  replicate:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        i: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]
        j: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]
    env:
      SOAK_SECONDS: "180"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Compute REPL_ID and RUN_DIR
        run: |
          I=${{ matrix.i }}
          J=${{ matrix.j }}
          REPL_ID=$(( ( (I-1)*16 ) + J ))
          echo "REPL_ID=$REPL_ID" >> $GITHUB_ENV
          echo "RUN_DIR=$GITHUB_WORKSPACE/runs/${REPL_ID}" >> $GITHUB_ENV

      - name: Ensure run output dirs
        run: |
          set -eu
          mkdir -p "${RUN_DIR}/collector"
          mkdir -p "${RUN_DIR}/site/data"

      - name: Install package (editable)
        run: |
          python -m pip install --upgrade pip
          python -m pip install -e .

      - name: Capture & Analyze
        shell: bash
        env:
          DEMO_DIR: ${{ github.workspace }}/vendor/opentelemetry-demo
        run: |
          set -euo pipefail
          mkdir -p "$RUN_DIR/collector"
          DEMO_DIR="${DEMO_DIR:-"$GITHUB_WORKSPACE/vendor/opentelemetry-demo"}"

          # 1) Get the OpenTelemetry Demo at a shallow depth (fast, deterministic)
          if [[ ! -d vendor/opentelemetry-demo ]]; then
            git clone --depth=1 https://github.com/open-telemetry/opentelemetry-demo vendor/opentelemetry-demo
          fi

          # 2) Enable a file exporter in the demo's "extras" config so traces are written to /data/otel-traces.json
          EXTRAS="${DEMO_DIR}/src/otel-collector/otelcol-config-extras.yml"
          
          # Drop only the exporters→service block to avoid conflicts, then append our config.
          awk 'BEGIN{p=1} /^exporters:/{p=0} /^service:/{print; p=1; next} p{print}' "$EXTRAS" > "$EXTRAS.tmp" && mv "$EXTRAS.tmp" "$EXTRAS"
          
          cat >> "$EXTRAS" <<'YAML'
          exporters:
            file/otlpjson:
              path: /data/otel-traces.json
              flush_interval: 2s
          
          service:
            pipelines:
              traces:
                exporters: [spanmetrics, otlp, file/otlpjson]
          YAML
        
          # 3) Compose override to mount a host directory for /data so the JSON lands in $RUN_DIR/collector
          cat > "${DEMO_DIR}/docker-compose.override.yml" <<'YAML'
          services:
            otel-collector:
              volumes:
                - "${RUN_DIR}/collector:/data:rw"
          YAML

        
          # 4) Bring the demo up, soak, then tear it down
          mkdir -p "${RUN_DIR}/collector"
          # ensure the collector (non-root in container) can create the file
          chmod 0777 "${RUN_DIR}/collector"

          COMPOSE="$DEMO_DIR/docker-compose.yml"
          test -f "$COMPOSE" || COMPOSE="$DEMO_DIR/compose.yaml"
          
          docker compose -f "$COMPOSE" up --force-recreate --remove-orphans --detach
          echo "Soaking for ${SOAK_SECONDS}s..."
          sleep "${SOAK_SECONDS}"
          
          # show collector logs (first) for easy diagnosis
          docker compose -f "$COMPOSE" logs --no-color --tail=2000 otel-collector || true
          
          test -s "${RUN_DIR}/collector/otel-traces.json" || {
            echo "ERROR: $(ls -lah ${RUN_DIR}/collector || true)"
            exit 1
          }

          docker compose -f "$COMPOSE" down -v || true
        
          # 5) Verify trace JSON and extract to Parquet
          if [[ ! -s "${RUN_DIR}/collector/otel-traces.json" ]]; then
          echo "ERROR: ${RUN_DIR}/collector/otel-traces.json is missing or empty."
          ls -l "${RUN_DIR}/collector" || true
          exit 1
          fi
          edgetyper extract --input "${RUN_DIR}/collector/otel-traces.json" --out "${RUN_DIR}/spans.parquet"
          echo "[capture] spans.parquet: $(ls -lh "${RUN_DIR}/spans.parquet" | awk '{print $5}')"
        
          # 6) Continue exactly as before (graph → features → predictions → eval → plan → observe)
          edgetyper graph \
          --spans "${RUN_DIR}/spans.parquet" \
          --out-events "${RUN_DIR}/events.parquet" \
          --out-edges "${RUN_DIR}/edges.parquet" \
          --with-broker-edges
          
          edgetyper featurize \
          --events "${RUN_DIR}/events.parquet" \
          --edges  "${RUN_DIR}/edges.parquet" \
          --out    "${RUN_DIR}/features.parquet"
          
          edgetyper baseline --features "${RUN_DIR}/features.parquet" --mode semconv --out "${RUN_DIR}/pred_semconv.csv"
          edgetyper baseline --features "${RUN_DIR}/features.parquet" --mode timing  --out "${RUN_DIR}/pred_timing.csv"
          edgetyper label    --features "${RUN_DIR}/features.parquet"              --out "${RUN_DIR}/pred_ours.csv"
          
          # Robustness masks
          edgetyper featurize --events "${RUN_DIR}/events.parquet" --edges "${RUN_DIR}/edges.parquet" --out "${RUN_DIR}/features_semconv_drop.parquet" --mask-semconv
          edgetyper label    --features "${RUN_DIR}/features_semconv_drop.parquet" --out "${RUN_DIR}/pred_ours_semconv_drop.csv" --uncertain-threshold 0.55
          
          edgetyper featurize --events "${RUN_DIR}/events.parquet" --edges "${RUN_DIR}/edges.parquet" --out "${RUN_DIR}/features_timing_drop.parquet" --mask-timing
          edgetyper label    --features "${RUN_DIR}/features_timing_drop.parquet" --out "${RUN_DIR}/pred_ours_timing_drop.csv" --uncertain-threshold 0.55
          
          # Physical GT path in this repo
          GT="src/edgetyper/ground_truth.yaml"
          
          edgetyper eval --name "Baseline — SemConv (physical)"   --pred "${RUN_DIR}/pred_semconv.csv"             --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_semconv_physical.json"
          edgetyper eval --name "Baseline — Timing (physical)"    --pred "${RUN_DIR}/pred_timing.csv"              --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_timing_physical.json"
          edgetyper eval --name "EdgeTyper (ours) — physical"     --pred "${RUN_DIR}/pred_ours.csv"                --features "${RUN_DIR}/features.parquet"              --gt "$GT" --out "${RUN_DIR}/metrics_ours_physical.json"
          edgetyper eval --name "EdgeTyper (ours) — SemConv dropped" --pred "${RUN_DIR}/pred_ours_semconv_drop.csv" --features "${RUN_DIR}/features_semconv_drop.parquet" --gt "$GT" --out "${RUN_DIR}/metrics_ours_semconv_drop.json"
          edgetyper eval --name "EdgeTyper (ours) — Timing dropped"  --pred "${RUN_DIR}/pred_ours_timing_drop.csv"  --features "${RUN_DIR}/features_timing_drop.parquet"  --gt "$GT" --out "${RUN_DIR}/metrics_ours_timing_drop.json"
          
          # Plans: ours vs all-blocking synthetic
          edgetyper plan --edges "${RUN_DIR}/edges.parquet" --pred "${RUN_DIR}/pred_ours.csv" --out "${RUN_DIR}/plan_physical.csv" --weight events --alpha-ack 1.0
          python - <<'PY'
          import pandas as pd, sys
          edges = pd.read_parquet(sys.argv[1])[["src_service","dst_service"]].drop_duplicates()
          edges["pred_label"]="sync"
          edges.to_csv(sys.argv[2], index=False)
          PY

          edgetyper plan --edges "${RUN_DIR}/edges.parquet" --pred "${RUN_DIR}/pred_all_blocking.csv" --out "${RUN_DIR}/plan_all_blocking.csv" --weight events --alpha-ack 1.0
          
          # Observe: build equal windows (baseline and two fault windows)
          python - <<'PY'
          import json, pandas as pd, os
          from pathlib import Path
          run = Path(os.environ["RUN_DIR"])
          sp  = pd.read_parquet(run/"spans.parquet")
          ts  = [c for c in ["start_time_unix_nano","start_unix_nano","start_ns","time_unix_nano"] if c in sp.columns][0]
          tmin, tmax = int(sp[ts].min()), int(sp[ts].max())
          W = 120 * 1_000_000_000
          segments = [
          { "name": "baseline",       "start_ns": tmax-3*W, "end_ns": tmax-2*W },
          { "name": "fault:kafka",    "target_service": "kafka",             "start_ns": tmax-2*W, "end_ns": tmax-1*W },
          { "name": "fault:consumer", "target_service": "fraud-detection",   "start_ns": tmax-1*W, "end_ns": tmax-0*W },
          ]
            (run/"segments.json").write_text(json.dumps({"segments":segments}, indent=2))
            PY
          edgetyper observe --spans "${RUN_DIR}/spans.parquet" --segments "${RUN_DIR}/segments.json" --out "${RUN_DIR}/observations.json"
      
          # Provenance (report size from JSON or fallback to Parquet)
          SIZE_MB=$(python - <<'PY'
          import os, json
          run=os.environ["RUN_DIR"]
          tj=os.path.join(run,"collector","otel-traces.json")
          pq=os.path.join(run,"spans.parquet")
          sz=os.path.getsize(tj) if os.path.exists(tj) else (os.path.getsize(pq) if os.path.exists(pq) else 0)
          print(round(sz/1024/1024,2))
          PY
          )
          cat > "${RUN_DIR}/provenance.json" <<JSON
          { "target": "otel-demo","soak_seconds": ${ SOAK_SECONDS },"traces_size_mb": ${ SIZE_MB } }
          JSON

      - name: Upload replica artifact
        uses: actions/upload-artifact@v4
        with:
          name: replicate-${{ env.REPL_ID }}
          path: |
            ${{ env.RUN_DIR }}/metrics_*.json
            ${{ env.RUN_DIR }}/plan_physical.csv
            ${{ env.RUN_DIR }}/plan_all_blocking.csv
            ${{ env.RUN_DIR }}/observations.json
            ${{ env.RUN_DIR }}/provenance.json

  aggregate:
    runs-on: ubuntu-latest
    needs: [replicate]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download all replica artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: replicate-*
          merge-multiple: true
          path: replicas

      - name: Aggregate replicas → site/
        run: |
          python scripts/aggregate_replicas.py --replicas-dir replicas --outdir site

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: [aggregate]
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
